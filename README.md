# Kotara Framework

<p align="center">
  </p>

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python Version](https://img.shields.io/badge/Python-3.8%2B-brightgreen.svg)](https://www.python.org/)


> Um framework de fus√£o adaptativa para combinar dois modelos de linguagem causal (LLMs) e avaliar a perplexidade (PPL) resultante.

**Kotara** √© uma ferramenta de pesquisa que explora a quest√£o: "A 'mente' combinada de dois LLMs pode superar o desempenho de cada um individualmente?". Inspirado na fus√£o Potara, este framework permite combinar as previs√µes de dois modelos em tempo real usando diversas estrat√©gias, desde uma simples m√©dia at√© m√©todos adaptativos baseados na confian√ßa de cada modelo.

---

## üìñ Sum√°rio

* [ Sobre o Projeto](#-sobre-o-projeto)
* [ Resultados Preliminares e Heur√≠sticas](#-resultados-preliminares-e-heur√≠sticas)
* [ Recursos](#-recursos)
* [ Instala√ß√£o](#-instala√ß√£o)
* [ Como Usar](#-como-usar)
* [ As Estrat√©gias de Fus√£o](#-as-estrat√©gias-de-fus√£o)
* [ Roadmap](#Ô∏è-roadmap)
* [ Licen√ßa](#-licen√ßa)
* [ Autor](#Ô∏è-autor)

---

## ‚ú® Sobre o Projeto

O Kotara nasce da vis√£o de poder fundir m√∫ltiplos modelos de linguagem em uma √∫nica entidade coesa, capaz de aprimorar o desempenho em tarefas downstream. A abordagem principal √© um framework modular que opera atrav√©s da **fus√£o de logits em tempo real**, sem a necessidade de modificar ou mesclar os pesos dos modelos, tornando o processo leve e flex√≠vel.

Por enquanto, o framework foca na avalia√ß√£o intr√≠nseca via **Perplexidade (PPL)**, mas a arquitetura √© projetada para ser extens√≠vel.

√â importante notar que esta √© uma **vers√£o beta**, e os experimentos iniciais foram conduzidos em condi√ß√µes espec√≠ficas. Isso significa que os resultados podem n√£o generalizar para todos os cen√°rios ou pares de modelos. Vers√µes futuras buscar√£o mitigar essas limita√ß√µes e aprimorar o desempenho da fus√£o em uma gama mais ampla de tarefas.

##  Resultados Preliminares e Heur√≠sticas

Testes realizados pelo **GTLM Research** com este framework, embora limitados, revelaram alguns insights e heur√≠sticas iniciais que podem guiar futuras experimenta√ß√µes:

* **O Ponto Ideal de Similaridade:** Houve ganhos observ√°veis na PPL **se, e somente se**, os modelos possu√≠am um grau de similaridade arquitetural e n√£o provinham de dom√≠nios de conhecimento completamente distintos. A fus√£o parece se beneficiar de "perspectivas" diferentes sobre um conhecimento em comum.

* **Excesso de Similaridade:** Modelos id√™nticos ou extremamente similares n√£o apresentaram ganhos significativos. A fus√£o de `A + A` n√£o gera um `A` melhor, pois n√£o h√° informa√ß√£o nova sendo introduzida no processo.

* **Excesso de Diferen√ßa:** A fus√£o de modelos de dom√≠nios muito distintos pode ser **prejudicial** ao desempenho se n√£o houver uma base de conhecimento comum e robusta entre eles. A fus√£o pode gerar mais "ru√≠do" do que "sinal".

> **Aviso:** Devido a limita√ß√µes de tempo e recursos, uma explora√ß√£o exaustiva para definir as melhores heur√≠sticas ainda n√£o foi conclu√≠da. Estes s√£o insights preliminares que servem como um ponto de partida para futuras investiga√ß√µes.

##  Recursos

* **Suporte Amplo:** Compat√≠vel com qualquer arquitetura `AutoModelForCausalLM` do Hugging Face (incluindo modelos que exigem `trust_remote_code=True`).
* **Flex√≠vel:** Avalie os modelos base individualmente e, em seguida, o resultado da fus√£o.
* **Estrat√©gias M√∫ltiplas:** Inclui quatro estrat√©gias de fus√£o na vers√£o beta: `average`, `poe`, `entropy` e `gap`.

---

##  Instala√ß√£o


```bash
# 1. Clone o reposit√≥rio
git clone [https://github.com/MadrasLe/kotara-framework.git](https://github.com/MadrasLe/kotara-framework.git)
cd kotara

# 2. (Recomendado) Crie um ambiente virtual
python -m venv .venv
source .venv/bin/activate  # No Windows: .venv\Scripts\activate

# 3. Instale as depend√™ncias
pip install -r requirements.txt

 Como Usar
O framework √© operado via linha de comando. Voc√™ precisar√° de dois modelos (locais ou do Hugging Face Hub) e um arquivo de texto (.txt) para avalia√ß√£o.

Exemplo de uso b√°sico (fus√£o com m√©dia simples):

Bash

python kotara.py \
    --model_a "gpt2" \
    --model_b "distilgpt2" \
    --dataset_path "./caminho/para/seu_texto.txt" \
    --strategy "average" \
    --save_metrics "resultados_average.json"

Bash

python kotara.py \
    --model_a "/path/to/your/local_model_A" \
    --model_b "EleutherAI/gpt-neo-125M" \
    --dataset_path "./data/wiki_pt.txt" \
    --strategy "entropy" \
    --temp_a 0.85 \
    --temp_b 1.1 \
    --dtype "bfloat16" \
    --max_length 2048 \
    --stride 1024 \
    --save_metrics "resultados_entropy.json"

Ap√≥s a execu√ß√£o, um relat√≥rio ser√° impresso no console e as m√©tricas detalhadas ser√£o salvas no arquivo JSON especificado.

## üß† As Estrat√©gias de Fus√£o

O Kotara permite escolher entre diferentes l√≥gicas para combinar os modelos. Cada uma tem uma filosofia diferente sobre como extrair o melhor de ambos os "especialistas".

* **`average`**: A abordagem mais direta e um excelente baseline. Os logits (as previs√µes brutas) dos dois modelos s√£o simplesmente somados e divididos por dois. √â como tirar a m√©dia da opini√£o de dois especialistas.

* **`poe` (Product of Experts)**: Uma t√©cnica mais sofisticada onde as probabilidades dos modelos s√£o multiplicadas (ou, de forma equivalente, seus log-propor√ß√µes s√£o somados). Este m√©todo tende a produzir previs√µes mais "pontiagudas" e confiantes, especialmente quando ambos os modelos concordam fortemente em uma previs√£o.

* **`entropy`**: Esta √© uma estrat√©gia de fus√£o **adaptativa** que funciona como um "supervisor de confian√ßa". Para cada token a ser previsto, ela mede a **Entropia de Shannon** da distribui√ß√£o de probabilidade de cada modelo.
    * **O que √© Entropia?** Pense nela como uma medida de "incerteza" ou "surpresa". Uma entropia baixa significa que o modelo est√° muito confiante em sua previs√£o (ex: 90% de chance para uma palavra). Uma entropia alta significa que o modelo est√° incerto (ex: as probabilidades est√£o muito espalhadas entre v√°rias palavras).
    * **Como funciona?** A estrat√©gia `entropy` d√° um peso maior ao modelo que apresenta a **menor incerteza** para aquele token espec√≠fico. O peso √© literalmente `1 / incerteza`. Assim, o "especialista" mais seguro para cada palavra ganha mais voz na decis√£o final.

* **`gap`**: Outra estrat√©gia **adaptativa** que mede a confian√ßa de uma forma diferente e muito intuitiva: a "margem de vit√≥ria" da melhor previs√£o.
    * **O que √© o "Gap"?** √â a diferen√ßa de probabilidade entre a palavra mais prov√°vel (top-1) e a segunda mais prov√°vel (top-2).
    * **Como funciona?** Um "gap" grande significa que o modelo n√£o tem d√∫vidas; sua melhor aposta se destaca claramente das outras. Um "gap" pequeno indica que o modelo est√° dividido entre duas ou mais op√ß√µes. A estrat√©gia `gap` d√° um peso maior ao modelo que tem a **decis√£o mais clara e inequ√≠voca**, ou seja, o maior "gap". Ele recompensa a decis√£o, n√£o apenas a confian√ßa.

 Roadmap
Esta √© a vers√£o beta do Kotara. O plano para a v1.0 e al√©m inclui:

[ ] Adicionar mais estrat√©gias de fus√£o.

[ ] Implementar um modo de "batch processing" para avalia√ß√£o ainda mais r√°pida.

[ ] Suporte para fus√£o de mais de dois modelos.

[ ] Cria√ß√£o de um pacote pip para f√°cil instala√ß√£o.

[ ] Adicionar exemplos em um notebook Jupyter.


 Licen√ßa
Este projeto √© distribu√≠do sob a Licen√ßa Apache 2.0. Veja o arquivo LICENSE para mais detalhes.

 Autor
Gabriel (MadrasLe)

GitHub: @MadrasLe

Um projeto da GTLM Research.
